# app/services/llm_service.py

from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate

# ì–¸ì–´ ëª¨ë¸ ì´ˆê¸°í™”
print("ğŸ”§ ì–¸ì–´ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
prompt_template = ChatPromptTemplate.from_messages(
    [
        SystemMessagePromptTemplate.from_template(
            """
            ë‹¹ì‹ ì€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì´ STT(ìŒì„± ì¸ì‹)ë¡œ ë³€í™˜ëœ í…ìŠ¤íŠ¸ë¼ì„œ ì–¸ì–´ê°€ ì„ì´ê±°ë‚˜ ì˜¤íƒ€ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ì„ ì •ì œí•˜ê³ , ì •ì œëœ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ëª…í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
            
            êµìˆ˜ê°€ ë‹µë³€í•œ ì›ë³¸ ì§ˆë¬¸ : {original_question}
            
            êµìˆ˜ê°€ ë‹µë³€í•œ ì›ë³¸ ë‹µë³€: {user_question}
            
            êµìˆ˜ê°€ ë‹µë³€í•œ ì›ë³¸ ì§ˆë¬¸-ë‹µë³€ì— ê¸°ë°˜í•˜ì—¬ ëª…í™•í•˜ê³  ê°„ê²°í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
            """
        ),
        HumanMessagePromptTemplate.from_template("{user_question}"),
    ]
)


def rag_pipeline(stt_input: str, user_question: str) -> dict:
    # STT ì…ë ¥ í…ìŠ¤íŠ¸ ì •ì œ
    stt_input

    # LLMì— ì •ì œëœ ì§ˆë¬¸ ì „ë‹¬
    print("ğŸ’¬ LLMì„ í†µí•œ ë‹µë³€ ìƒì„± ì¤‘...")
    prompt = prompt_template.format(
        original_question=stt_input, user_question=user_question
    )
    output = llm.invoke(prompt)

    return {"LLMAnswer": output.content, "userQuestion": user_question}
