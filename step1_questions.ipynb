{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders import UnstructuredFileLoader \n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "from langchain.utils.math import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Initializing language model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seochan/ai-server/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Setting up cache directory...\n",
      "âœ‚ï¸ Setting up text splitter...\n",
      "ğŸ“„ Loading document...\n",
      "ğŸ” Splitting document into chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seochan/ai-server/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `UnstructuredFileLoader` was deprecated in LangChain 0.2.8 and will be removed in 0.4.0. An updated version of the class exists in the langchain-unstructured package and should be used instead. To use it run `pip install -U langchain-unstructured` and import as `from langchain_unstructured import UnstructuredLoader`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Initializing embeddings model...\n",
      "ğŸ—‚ï¸ Setting up cached embeddings...\n",
      "ğŸ“Š Creating vector store from documents...\n",
      "ğŸ” Setting up retriever...\n",
      "âœ… Process completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seochan/ai-server/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# ì–¸ì–´ ëª¨ë¸ ì´ˆê¸°í™”\n",
    "print(\"ğŸ”§ Initializing language model...\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
    "\n",
    "# ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "print(\"ğŸ’¾ Setting up cache directory...\")\n",
    "cache_dir = LocalFileStore(\"./.cache/embeddings/course_computer_networks\")\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„° ì„¤ì •\n",
    "print(\"âœ‚ï¸ Setting up text splitter...\")\n",
    "spliter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "# ë¬¸ì„œ ë¡œë“œ\n",
    "print(\"ğŸ“„ Loading document...\")\n",
    "loader = UnstructuredFileLoader(\"./dummy.csv\")\n",
    "\n",
    "# ë¬¸ì„œ ë¶„í• \n",
    "print(\"ğŸ” Splitting document into chunks...\")\n",
    "docs = loader.load_and_split(text_splitter=spliter)\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ì„¤ì •\n",
    "print(\"ğŸ§  Initializing embeddings model...\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# ìºì‹± ì„ë² ë”© ì„¤ì •\n",
    "print(\"ğŸ—‚ï¸ Setting up cached embeddings...\")\n",
    "cache_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,\n",
    "    cache_dir,\n",
    ")\n",
    "\n",
    "# ë²¡í„°ìŠ¤í† ì–´ ìƒì„±\n",
    "print(\"ğŸ“Š Creating vector store from documents...\")\n",
    "vectorstore = FAISS.from_documents(docs, cache_embeddings)\n",
    "\n",
    "# ê²€ìƒ‰ê¸° ì„¤ì •\n",
    "print(\"ğŸ” Setting up retriever...\")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "print(\"âœ… Process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            ë‹¹ì‹ ì€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. \n",
    "            ì‚¬ìš©ìê°€ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤.\n",
    "            \n",
    "            ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ì§ˆì˜ì‘ë‹µ ì„  ì‚¬ë¡€ : {context}\n",
    "\n",
    "            ìœ„ ì„ ì‚¬ë¡€ì— ê¸°ë°˜í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(question):\n",
    "    # ì…ë ¥ëœ ì§ˆë¬¸ì— ëŒ€í•œ ì„ë² ë”© ìƒì„±\n",
    "    print(\"ğŸ§  Embedding the input question...\")\n",
    "    question_embedding = embeddings.embed_documents([question])[0]    \n",
    "    # ê¸°ì¡´ ì§ˆë¬¸ê³¼ ë¹„êµ\n",
    "    print(\"ğŸ” Comparing with existing documents...\")\n",
    "    results = retriever.get_relevant_documents(question)\n",
    "    similarities = {}\n",
    "    \n",
    "    for doc in results:\n",
    "        doc_content = doc.page_content.strip()\n",
    "        if not doc_content:\n",
    "            continue\n",
    "        \n",
    "        doc_embedding = embeddings.embed_documents([doc_content])[0]\n",
    "        similarity = cosine_similarity([question_embedding], [doc_embedding])[0][0]\n",
    "        similarity_percentage = similarity * 100\n",
    "        similarities[doc_content] = similarity_percentage\n",
    "\n",
    "    # ê°€ì¥ ìœ ì‚¬í•œ ì§ˆë¬¸ ì°¾ê¸°\n",
    "    most_similar_question = max(similarities, key=similarities.get, default=None)\n",
    "    if most_similar_question:\n",
    "        max_similarity = similarities[most_similar_question]\n",
    "        \n",
    "\n",
    "        if max_similarity > 80.0:  # ìœ ì‚¬ë„ê°€ ì¶©ë¶„íˆ ë†’ì€ ê²½ìš° ê´€ë ¨ ë‹µë³€ ì‚¬ìš©\n",
    "            context = f\"ì§ˆë¬¸ì´ '{most_similar_question}'ì™€ ë§¤ìš° ìœ ì‚¬í•©ë‹ˆë‹¤.\"\n",
    "            print(f\"\\nê°€ì¥ ìœ ì‚¬í•œ ì§ˆë¬¸: '{most_similar_question}'\")\n",
    "            print(f\"ìœ ì‚¬ë„: {max_similarity:.2f}%\")\n",
    "            prompt = prompt_template.format(context=context, question=question)\n",
    "            output = llm.invoke(prompt)\n",
    "            return { \"LLMAnswer\": output.content, \"userQuestion\": question}\n",
    "    \n",
    "    return {\"LLMAnswer\": None, \"userQuestion\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Embedding the input question...\n",
      "ğŸ” Comparing with existing documents...\n",
      "Assistant: {'LLMAnswer': None, 'userQuestion': 'MACì— ëŒ€í•´ì„œ ìì„¸íˆ ì•Œë ¤ì£¼ì„¸ìš”.'}\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "ğŸ§  Embedding the input question...\n",
      "ğŸ” Comparing with existing documents...\n",
      "Assistant: {'LLMAnswer': None, 'userQuestion': 'ì»´íŒŒì¼ëŸ¬ê°€ ë¬´ì—‡ì¸ê°€ìš”??'}\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "ğŸ§  Embedding the input question...\n",
      "ğŸ” Comparing with existing documents...\n",
      "\n",
      "ê°€ì¥ ìœ ì‚¬í•œ ì§ˆë¬¸: '5\n",
      "2024-08-20\n",
      "ìŠ¤ìœ„ì¹˜ì™€ í—ˆë¸Œì˜ ì°¨ì´ì ì€?\n",
      "ìŠ¤ìœ„ì¹˜ëŠ” ë°ì´í„° íŒ¨í‚·ì„ íŠ¹ì • ì¥ì¹˜ë¡œ ì „ë‹¬í•˜ëŠ” ë°˜ë©´, í—ˆë¸ŒëŠ” ëª¨ë“  ì¥ì¹˜ë¡œ ë°ì´í„°ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤.\n",
      "6\n",
      "2024-08-20\n",
      "IP ì£¼ì†Œë€?\n",
      "IP ì£¼ì†ŒëŠ” ë„¤íŠ¸ì›Œí¬ ìƒì—ì„œ ì¥ì¹˜ë“¤ì„ ì‹ë³„í•˜ê¸° ìœ„í•œ ê³ ìœ  ì£¼ì†Œì…ë‹ˆë‹¤.\n",
      "7\n",
      "2024-08-20\n",
      "DNSì˜ ì—­í• ì€?\n",
      "DNSëŠ” ë„ë©”ì¸ ì´ë¦„ì„ IP ì£¼ì†Œë¡œ ë³€í™˜í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n",
      "8\n",
      "2024-08-20\n",
      "ë°©í™”ë²½ì´ë€?\n",
      "ë°©í™”ë²½ì€ ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆì„ ìœ„í•´ ì™¸ë¶€ì˜ ë¶ˆë²•ì ì¸ ì ‘ê·¼ì„ ì°¨ë‹¨í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n",
      "9\n",
      "2024-08-20\n",
      "VPNì˜ ê¸°ëŠ¥ì€?\n",
      "VPNì€ ê³µìš© ë„¤íŠ¸ì›Œí¬ì—ì„œ ì‚¬ì„¤ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì•ˆì„ ì œê³µí•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
      "10\n",
      "2024-08-20\n",
      "íŒ¨í‚·ì´ë€?'\n",
      "ìœ ì‚¬ë„: 85.97%\n",
      "Assistant: {'LLMAnswer': 'TCP(Transmission Control Protocol)ì™€ UDP(User Datagram Protocol)ëŠ” ë‘˜ ë‹¤ ì¸í„°ë„· í”„ë¡œí† ì½œ ìŠ¤ìœ„íŠ¸ì˜ ì¼ë¶€ë¡œ, ë°ì´í„° ì „ì†¡ì„ ìœ„í•œ í”„ë¡œí† ì½œì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ë‘ í”„ë¡œí† ì½œì€ ë°ì´í„° ì „ì†¡ ë°©ì‹ì—ì„œ ì—¬ëŸ¬ ê°€ì§€ ì¤‘ìš”í•œ ì°¨ì´ì ì´ ìˆìŠµë‹ˆë‹¤.\\n\\n1. **ì—°ê²° ì§€í–¥ vs ë¹„ì—°ê²° ì§€í–¥**:\\n   - **TCP**: ì—°ê²° ì§€í–¥ í”„ë¡œí† ì½œë¡œ, ë°ì´í„° ì „ì†¡ ì „ì— ì†¡ì‹ ìì™€ ìˆ˜ì‹ ì ê°„ì— ì—°ê²°ì„ ì„¤ì •í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ë°ì´í„°ì˜ ì‹ ë¢°ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.\\n   - **UDP**: ë¹„ì—°ê²° ì§€í–¥ í”„ë¡œí† ì½œë¡œ, ì—°ê²°ì„ ì„¤ì •í•˜ì§€ ì•Šê³  ë°ì´í„°ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ì „ì†¡ ì†ë„ê°€ ë¹ ë¥´ì§€ë§Œ, ì‹ ë¢°ì„±ì€ ë‚®ìŠµë‹ˆë‹¤.\\n\\n2. **ì‹ ë¢°ì„±**:\\n   - **TCP**: ë°ì´í„°ê°€ ì†ì‹¤ë˜ê±°ë‚˜ ìˆœì„œê°€ ë°”ë€Œë©´ ì¬ì „ì†¡ì„ ìš”ì²­í•˜ì—¬ ë°ì´í„°ì˜ ì •í™•ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì‹ ë¢°ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\\n   - **UDP**: ë°ì´í„°ê°€ ì†ì‹¤ë˜ê±°ë‚˜ ìˆœì„œê°€ ë°”ë€Œì–´ë„ ì´ë¥¼ í™•ì¸í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì‹ ë¢°ì„±ì´ ë‚®ìŠµë‹ˆë‹¤. ì‹¤ì‹œê°„ ì „ì†¡ì´ í•„ìš”í•œ ê²½ìš°ì— ì£¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n\\n3. **ì „ì†¡ ì†ë„**:\\n   - **TCP**: ì—°ê²° ì„¤ì • ë° ë°ì´í„° í™•ì¸ ê³¼ì •ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— ìƒëŒ€ì ìœ¼ë¡œ ì „ì†¡ ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n   - **UDP**: ì—°ê²° ì„¤ì •ì´ ì—†ê³  ë°ì´í„° í™•ì¸ ê³¼ì •ì´ ì—†ê¸° ë•Œë¬¸ì— ì „ì†¡ ì†ë„ê°€ ë¹ ë¦…ë‹ˆë‹¤.\\n\\n4. **ìš©ë„**:\\n   - **TCP**: ì›¹ ë¸Œë¼ìš°ì§•, ì´ë©”ì¼, íŒŒì¼ ì „ì†¡ ë“± ë°ì´í„°ì˜ ì •í™•ì„±ì´ ì¤‘ìš”í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n   - **UDP**: ì˜¨ë¼ì¸ ê²Œì„, ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë°, VoIP(Voice over IP) ë“± ì‹¤ì‹œê°„ ì „ì†¡ì´ ì¤‘ìš”í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n\\nì´ëŸ¬í•œ ì°¨ì´ì ìœ¼ë¡œ ì¸í•´ TCPì™€ UDPëŠ” ê°ê°ì˜ ìš©ë„ì— ë§ê²Œ ì„ íƒë˜ì–´ ì‚¬ìš©ë©ë‹ˆë‹¤.', 'userQuestion': 'TCPì™€ UDPì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?'}\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "ğŸ§  Embedding the input question...\n",
      "ğŸ” Comparing with existing documents...\n",
      "Assistant: {'LLMAnswer': None, 'userQuestion': 'ë°ì´í„°ë² ì´ìŠ¤ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.'}\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "ğŸ§  Embedding the input question...\n",
      "ğŸ” Comparing with existing documents...\n",
      "\n",
      "ê°€ì¥ ìœ ì‚¬í•œ ì§ˆë¬¸: '5\n",
      "2024-08-20\n",
      "ìŠ¤ìœ„ì¹˜ì™€ í—ˆë¸Œì˜ ì°¨ì´ì ì€?\n",
      "ìŠ¤ìœ„ì¹˜ëŠ” ë°ì´í„° íŒ¨í‚·ì„ íŠ¹ì • ì¥ì¹˜ë¡œ ì „ë‹¬í•˜ëŠ” ë°˜ë©´, í—ˆë¸ŒëŠ” ëª¨ë“  ì¥ì¹˜ë¡œ ë°ì´í„°ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤.\n",
      "6\n",
      "2024-08-20\n",
      "IP ì£¼ì†Œë€?\n",
      "IP ì£¼ì†ŒëŠ” ë„¤íŠ¸ì›Œí¬ ìƒì—ì„œ ì¥ì¹˜ë“¤ì„ ì‹ë³„í•˜ê¸° ìœ„í•œ ê³ ìœ  ì£¼ì†Œì…ë‹ˆë‹¤.\n",
      "7\n",
      "2024-08-20\n",
      "DNSì˜ ì—­í• ì€?\n",
      "DNSëŠ” ë„ë©”ì¸ ì´ë¦„ì„ IP ì£¼ì†Œë¡œ ë³€í™˜í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n",
      "8\n",
      "2024-08-20\n",
      "ë°©í™”ë²½ì´ë€?\n",
      "ë°©í™”ë²½ì€ ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆì„ ìœ„í•´ ì™¸ë¶€ì˜ ë¶ˆë²•ì ì¸ ì ‘ê·¼ì„ ì°¨ë‹¨í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.\n",
      "9\n",
      "2024-08-20\n",
      "VPNì˜ ê¸°ëŠ¥ì€?\n",
      "VPNì€ ê³µìš© ë„¤íŠ¸ì›Œí¬ì—ì„œ ì‚¬ì„¤ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì•ˆì„ ì œê³µí•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
      "10\n",
      "2024-08-20\n",
      "íŒ¨í‚·ì´ë€?'\n",
      "ìœ ì‚¬ë„: 84.24%\n",
      "Assistant: {'LLMAnswer': \"TCP(Transmission Control Protocol)ì˜ ì´ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n\\n1. **ì‹ ë¢°ì„±**: TCPëŠ” ë°ì´í„° ì „ì†¡ì˜ ì‹ ë¢°ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤. ë°ì´í„°ê°€ ì†ì‹¤ë˜ê±°ë‚˜ ì†ìƒë˜ì—ˆì„ ê²½ìš°, TCPëŠ” í•´ë‹¹ ë°ì´í„°ë¥¼ ì¬ì „ì†¡í•˜ì—¬ ì™„ì „í•œ ë°ì´í„° ì „ì†¡ì„ ë³´ì¥í•©ë‹ˆë‹¤.\\n\\n2. **ìˆœì„œ ë³´ì¥**: TCPëŠ” ë°ì´í„° íŒ¨í‚·ì´ ì „ì†¡ëœ ìˆœì„œëŒ€ë¡œ ìˆ˜ì‹ ë˜ë„ë¡ í•©ë‹ˆë‹¤. ì´ëŠ” ìˆ˜ì‹  ì¸¡ì—ì„œ ë°ì´í„°ê°€ ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ì¡°ë¦½ë  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.\\n\\n3. **íë¦„ ì œì–´**: TCPëŠ” ì†¡ì‹ ìì™€ ìˆ˜ì‹ ì ê°„ì˜ ë°ì´í„° ì „ì†¡ ì†ë„ë¥¼ ì¡°ì ˆí•˜ì—¬ ìˆ˜ì‹ ìê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì–‘ë§Œí¼ë§Œ ë°ì´í„°ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë„¤íŠ¸ì›Œí¬ í˜¼ì¡ì„ ë°©ì§€í•©ë‹ˆë‹¤.\\n\\n4. **í˜¼ì¡ ì œì–´**: TCPëŠ” ë„¤íŠ¸ì›Œí¬ì˜ í˜¼ì¡ ìƒíƒœë¥¼ ê°ì§€í•˜ê³ , í˜¼ì¡í•  ë•Œ ë°ì´í„° ì „ì†¡ ì†ë„ë¥¼ ì¡°ì ˆí•˜ì—¬ ë„¤íŠ¸ì›Œí¬ì˜ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.\\n\\n5. **ì—°ê²° ì§€í–¥**: TCPëŠ” ë°ì´í„° ì „ì†¡ì„ ì‹œì‘í•˜ê¸° ì „ì— ì†¡ì‹ ìì™€ ìˆ˜ì‹ ì ê°„ì˜ ì—°ê²°ì„ ì„¤ì •í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ '3-way handshake'ë¼ê³  ë¶ˆë¦¬ë©°, ì•ˆì •ì ì¸ í†µì‹ ì„ ìœ„í•œ ê¸°ë°˜ì„ ì œê³µí•©ë‹ˆë‹¤.\\n\\nì´ëŸ¬í•œ ì´ì ë“¤ ë•ë¶„ì— TCPëŠ” ì›¹ ë¸Œë¼ìš°ì§•, ì´ë©”ì¼ ì „ì†¡, íŒŒì¼ ì „ì†¡ ë“± ì‹ ë¢°ì„±ì´ ì¤‘ìš”í•œ ë‹¤ì–‘í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.\", 'userQuestion': 'TCPì˜ ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?'}\n",
      "\n",
      "-----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ë©”ì¸ í•¨ìˆ˜\n",
    "def main(user_input):\n",
    "    response = rag_pipeline(user_input)\n",
    "    print(\"Assistant:\", response)\n",
    "\n",
    "user_input = [\n",
    "                \"MACì— ëŒ€í•´ì„œ ìì„¸íˆ ì•Œë ¤ì£¼ì„¸ìš”.\",\n",
    "                \"ì»´íŒŒì¼ëŸ¬ê°€ ë¬´ì—‡ì¸ê°€ìš”??\", \n",
    "                \"TCPì™€ UDPì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\", \n",
    "                \"ë°ì´í„°ë² ì´ìŠ¤ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.\",\n",
    "                \"TCPì˜ ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "            ]\n",
    "\n",
    "for i in user_input:\n",
    "    main(i)\n",
    "    print(\"\\n-----------------------------------\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì „ì²´ ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Initializing language model...\n",
      "ğŸ’¾ Setting up cache directory...\n",
      "âœ‚ï¸ Setting up text splitter...\n",
      "ğŸ“„ Loading document...\n",
      "ğŸ” Splitting document into chunks...\n",
      "ğŸ§  Initializing embeddings model...\n",
      "ğŸ—‚ï¸ Setting up cached embeddings...\n",
      "ğŸ“Š Creating vector store from documents...\n",
      "ğŸ” Setting up retriever...\n",
      "âœ… Process completed!\n",
      "ğŸ§  Embedding the input question...\n",
      "ğŸ” Comparing with existing documents...\n",
      "Assistant: {'LLMAnswer': 'MAC(Medium Access Control) ì£¼ì†ŒëŠ” ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤ ì¹´ë“œ(NIC)ì— í• ë‹¹ëœ ê³ ìœ í•œ ì‹ë³„ìì…ë‹ˆë‹¤. ì´ ì£¼ì†ŒëŠ” ë³´í†µ 48ë¹„íŠ¸(6ë°”ì´íŠ¸) ê¸¸ì´ë¡œ, 16ì§„ìˆ˜ë¡œ í‘œí˜„ë˜ë©°, ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤: `00:1A:2B:3C:4D:5E`.\\n\\nMAC ì£¼ì†ŒëŠ” ë„¤íŠ¸ì›Œí¬ì—ì„œ ì¥ì¹˜ë¥¼ ì‹ë³„í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, ì£¼ë¡œ ë¡œì»¬ ë„¤íŠ¸ì›Œí¬ ë‚´ì—ì„œ ë°ì´í„° íŒ¨í‚·ì´ ì˜¬ë°”ë¥¸ ì¥ì¹˜ë¡œ ì „ì†¡ë˜ë„ë¡ ë•ìŠµë‹ˆë‹¤. ê° ë„¤íŠ¸ì›Œí¬ ì¥ì¹˜ëŠ” ê³ ìœ í•œ MAC ì£¼ì†Œë¥¼ ê°€ì§€ë¯€ë¡œ, ë„¤íŠ¸ì›Œí¬ì—ì„œ ì¶©ëŒ ì—†ì´ ì—¬ëŸ¬ ì¥ì¹˜ê°€ ë™ì‹œì— í†µì‹ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\nMAC ì£¼ì†ŒëŠ” ë‘ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì²« ë²ˆì§¸ 3ë°”ì´íŠ¸ëŠ” OUI(Organizationally Unique Identifier)ë¡œ, ì œì¡°ì—…ì²´ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤. ë‚˜ë¨¸ì§€ 3ë°”ì´íŠ¸ëŠ” í•´ë‹¹ ì œì¡°ì—…ì²´ê°€ ë§Œë“  íŠ¹ì • ì¥ì¹˜ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤.\\n\\nMAC ì£¼ì†ŒëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë³€ê²½ë˜ì§€ ì•Šì§€ë§Œ, ì¼ë¶€ ë„¤íŠ¸ì›Œí¬ í™˜ê²½ì—ì„œëŠ” ë³´ì•ˆì´ë‚˜ ê´€ë¦¬ìƒì˜ ì´ìœ ë¡œ ì†Œí”„íŠ¸ì›¨ì–´ì ìœ¼ë¡œ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. MAC ì£¼ì†ŒëŠ” ì£¼ë¡œ ì´ë”ë„· ë„¤íŠ¸ì›Œí¬ì™€ Wi-Fi ë„¤íŠ¸ì›Œí¬ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.', 'userQuestion': 'MACì— ëŒ€í•´ì„œ ìì„¸íˆ ì•Œë ¤ì£¼ì„¸ìš”.'}\n",
      "\n",
      "-----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders import UnstructuredFileLoader \n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "from langchain.utils.math import cosine_similarity\n",
    "\n",
    "# ì–¸ì–´ ëª¨ë¸ ì´ˆê¸°í™”\n",
    "print(\"ğŸ”§ Initializing language model...\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
    "\n",
    "# ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "print(\"ğŸ’¾ Setting up cache directory...\")\n",
    "cache_dir = LocalFileStore(\"./.cache/embeddings/course_computer_networks\")\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„° ì„¤ì •\n",
    "print(\"âœ‚ï¸ Setting up text splitter...\")\n",
    "spliter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "# ë¬¸ì„œ ë¡œë“œ\n",
    "print(\"ğŸ“„ Loading document...\")\n",
    "loader = UnstructuredFileLoader(\"./dummy.csv\")\n",
    "\n",
    "# ë¬¸ì„œ ë¶„í• \n",
    "print(\"ğŸ” Splitting document into chunks...\")\n",
    "docs = loader.load_and_split(text_splitter=spliter)\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ì„¤ì •\n",
    "print(\"ğŸ§  Initializing embeddings model...\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# ìºì‹± ì„ë² ë”© ì„¤ì •\n",
    "print(\"ğŸ—‚ï¸ Setting up cached embeddings...\")\n",
    "cache_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,\n",
    "    cache_dir,\n",
    ")\n",
    "\n",
    "# ë²¡í„°ìŠ¤í† ì–´ ìƒì„±\n",
    "print(\"ğŸ“Š Creating vector store from documents...\")\n",
    "vectorstore = FAISS.from_documents(docs, cache_embeddings)\n",
    "\n",
    "# ê²€ìƒ‰ê¸° ì„¤ì •\n",
    "print(\"ğŸ” Setting up retriever...\")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "print(\"âœ… Process completed!\")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            ë‹¹ì‹ ì€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. \n",
    "            ì‚¬ìš©ìê°€ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤.\n",
    "            \n",
    "            ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ì§ˆì˜ì‘ë‹µ ì„  ì‚¬ë¡€ : {context}\n",
    "\n",
    "            ìœ„ ì„ ì‚¬ë¡€ì— ê¸°ë°˜í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "def rag_pipeline(question):\n",
    "    # ì…ë ¥ëœ ì§ˆë¬¸ì— ëŒ€í•œ ì„ë² ë”© ìƒì„±\n",
    "    print(\"ğŸ§  Embedding the input question...\")\n",
    "    question_embedding = embeddings.embed_documents([question])[0]    \n",
    "    # ê¸°ì¡´ ì§ˆë¬¸ê³¼ ë¹„êµ\n",
    "    print(\"ğŸ” Comparing with existing documents...\")\n",
    "    results = retriever.get_relevant_documents(question)\n",
    "    similarities = {}\n",
    "    \n",
    "    for doc in results:\n",
    "        doc_content = doc.page_content.strip()\n",
    "        if not doc_content:\n",
    "            continue\n",
    "        \n",
    "        doc_embedding = embeddings.embed_documents([doc_content])[0]\n",
    "        similarity = cosine_similarity([question_embedding], [doc_embedding])[0][0]\n",
    "        similarity_percentage = similarity * 100\n",
    "        similarities[doc_content] = similarity_percentage\n",
    "\n",
    "    # ê°€ì¥ ìœ ì‚¬í•œ ì§ˆë¬¸ ì°¾ê¸°\n",
    "    most_similar_question = max(similarities, key=similarities.get, default=None)\n",
    "    if most_similar_question:\n",
    "        max_similarity = similarities[most_similar_question]\n",
    "        \n",
    "\n",
    "        if max_similarity > 80.0:  # ìœ ì‚¬ë„ê°€ ì¶©ë¶„íˆ ë†’ì€ ê²½ìš° ê´€ë ¨ ë‹µë³€ ì‚¬ìš©\n",
    "            context = f\"ì§ˆë¬¸ì´ '{most_similar_question}'ì™€ ë§¤ìš° ìœ ì‚¬í•©ë‹ˆë‹¤.\"\n",
    "            # print(f\"\\nê°€ì¥ ìœ ì‚¬í•œ ì§ˆë¬¸: '{most_similar_question}'\")\n",
    "            # print(f\"ìœ ì‚¬ë„: {max_similarity:.2f}%\")\n",
    "            prompt = prompt_template.format(context=context, question=question)\n",
    "            output = llm.invoke(prompt)\n",
    "            return { \"LLMAnswer\": output.content, \"userQuestion\": question}\n",
    "    \n",
    "    return {\"LLMAnswer\": None, \"userQuestion\": question}\n",
    "\n",
    "# ë©”ì¸ í•¨ìˆ˜\n",
    "def main(user_input):\n",
    "    response = rag_pipeline(user_input)\n",
    "    print(\"Assistant:\", response)\n",
    "\n",
    "user_input = [\n",
    "                \"MACì— ëŒ€í•´ì„œ ìì„¸íˆ ì•Œë ¤ì£¼ì„¸ìš”.\",\n",
    "                # \"ì»´íŒŒì¼ëŸ¬ê°€ ë¬´ì—‡ì¸ê°€ìš”??\", \n",
    "                # \"TCPì™€ UDPì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\", \n",
    "                # \"ë°ì´í„°ë² ì´ìŠ¤ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.\",\n",
    "                # \"TCPì˜ ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "            ]\n",
    "\n",
    "for i in user_input:\n",
    "    main(i)\n",
    "    print(\"\\n-----------------------------------\\n\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
