{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders import UnstructuredFileLoader \n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "from langchain.utils.math import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Initializing language model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seochan/ai-server/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Setting up cache directory...\n",
      "✂️ Setting up text splitter...\n",
      "📄 Loading document...\n",
      "🔍 Splitting document into chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seochan/ai-server/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `UnstructuredFileLoader` was deprecated in LangChain 0.2.8 and will be removed in 0.4.0. An updated version of the class exists in the langchain-unstructured package and should be used instead. To use it run `pip install -U langchain-unstructured` and import as `from langchain_unstructured import UnstructuredLoader`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Initializing embeddings model...\n",
      "🗂️ Setting up cached embeddings...\n",
      "📊 Creating vector store from documents...\n",
      "🔎 Setting up retriever...\n",
      "✅ Process completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seochan/ai-server/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# 언어 모델 초기화\n",
    "print(\"🔧 Initializing language model...\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
    "\n",
    "# 캐시 디렉토리 설정\n",
    "print(\"💾 Setting up cache directory...\")\n",
    "cache_dir = LocalFileStore(\"./.cache/embeddings/course_computer_networks\")\n",
    "\n",
    "# 텍스트 스플리터 설정\n",
    "print(\"✂️ Setting up text splitter...\")\n",
    "spliter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "# 문서 로드\n",
    "print(\"📄 Loading document...\")\n",
    "loader = UnstructuredFileLoader(\"./dummy.csv\")\n",
    "\n",
    "# 문서 분할\n",
    "print(\"🔍 Splitting document into chunks...\")\n",
    "docs = loader.load_and_split(text_splitter=spliter)\n",
    "\n",
    "# 임베딩 모델 설정\n",
    "print(\"🧠 Initializing embeddings model...\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 캐싱 임베딩 설정\n",
    "print(\"🗂️ Setting up cached embeddings...\")\n",
    "cache_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,\n",
    "    cache_dir,\n",
    ")\n",
    "\n",
    "# 벡터스토어 생성\n",
    "print(\"📊 Creating vector store from documents...\")\n",
    "vectorstore = FAISS.from_documents(docs, cache_embeddings)\n",
    "\n",
    "# 검색기 설정\n",
    "print(\"🔎 Setting up retriever...\")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "print(\"✅ Process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿 생성\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            당신은 AI 어시스턴트입니다. \n",
    "            사용자가 질문에 대한 답변을 찾을 수 있도록 돕습니다.\n",
    "            \n",
    "            사용자 질문과 유사한 질의응답 선 사례 : {context}\n",
    "\n",
    "            위 선사례에 기반하여 사용자의 질문에 대한 답변을 친절하게 설명해주세요.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(question):\n",
    "    # 입력된 질문에 대한 임베딩 생성\n",
    "    print(\"🧠 Embedding the input question...\")\n",
    "    question_embedding = embeddings.embed_documents([question])[0]    \n",
    "    # 기존 질문과 비교\n",
    "    print(\"🔍 Comparing with existing documents...\")\n",
    "    results = retriever.get_relevant_documents(question)\n",
    "    similarities = {}\n",
    "    \n",
    "    for doc in results:\n",
    "        doc_content = doc.page_content.strip()\n",
    "        if not doc_content:\n",
    "            continue\n",
    "        \n",
    "        doc_embedding = embeddings.embed_documents([doc_content])[0]\n",
    "        similarity = cosine_similarity([question_embedding], [doc_embedding])[0][0]\n",
    "        similarity_percentage = similarity * 100\n",
    "        similarities[doc_content] = similarity_percentage\n",
    "\n",
    "    # 가장 유사한 질문 찾기\n",
    "    most_similar_question = max(similarities, key=similarities.get, default=None)\n",
    "    if most_similar_question:\n",
    "        max_similarity = similarities[most_similar_question]\n",
    "        \n",
    "\n",
    "        if max_similarity > 80.0:  # 유사도가 충분히 높은 경우 관련 답변 사용\n",
    "            context = f\"질문이 '{most_similar_question}'와 매우 유사합니다.\"\n",
    "            print(f\"\\n가장 유사한 질문: '{most_similar_question}'\")\n",
    "            print(f\"유사도: {max_similarity:.2f}%\")\n",
    "            prompt = prompt_template.format(context=context, question=question)\n",
    "            output = llm.invoke(prompt)\n",
    "            return { \"LLMAnswer\": output.content, \"userQuestion\": question}\n",
    "    \n",
    "    return {\"LLMAnswer\": None, \"userQuestion\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Embedding the input question...\n",
      "🔍 Comparing with existing documents...\n",
      "Assistant: {'LLMAnswer': None, 'userQuestion': 'MAC에 대해서 자세히 알려주세요.'}\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "🧠 Embedding the input question...\n",
      "🔍 Comparing with existing documents...\n",
      "Assistant: {'LLMAnswer': None, 'userQuestion': '컴파일러가 무엇인가요??'}\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "🧠 Embedding the input question...\n",
      "🔍 Comparing with existing documents...\n",
      "\n",
      "가장 유사한 질문: '5\n",
      "2024-08-20\n",
      "스위치와 허브의 차이점은?\n",
      "스위치는 데이터 패킷을 특정 장치로 전달하는 반면, 허브는 모든 장치로 데이터를 전송합니다.\n",
      "6\n",
      "2024-08-20\n",
      "IP 주소란?\n",
      "IP 주소는 네트워크 상에서 장치들을 식별하기 위한 고유 주소입니다.\n",
      "7\n",
      "2024-08-20\n",
      "DNS의 역할은?\n",
      "DNS는 도메인 이름을 IP 주소로 변환하는 시스템입니다.\n",
      "8\n",
      "2024-08-20\n",
      "방화벽이란?\n",
      "방화벽은 네트워크 보안을 위해 외부의 불법적인 접근을 차단하는 시스템입니다.\n",
      "9\n",
      "2024-08-20\n",
      "VPN의 기능은?\n",
      "VPN은 공용 네트워크에서 사설 네트워크를 사용하는 것처럼 보안을 제공하는 기술입니다.\n",
      "10\n",
      "2024-08-20\n",
      "패킷이란?'\n",
      "유사도: 85.97%\n",
      "Assistant: {'LLMAnswer': 'TCP(Transmission Control Protocol)와 UDP(User Datagram Protocol)는 둘 다 인터넷 프로토콜 스위트의 일부로, 데이터 전송을 위한 프로토콜입니다. 그러나 이 두 프로토콜은 데이터 전송 방식에서 여러 가지 중요한 차이점이 있습니다.\\n\\n1. **연결 지향 vs 비연결 지향**:\\n   - **TCP**: 연결 지향 프로토콜로, 데이터 전송 전에 송신자와 수신자 간에 연결을 설정합니다. 이 과정에서 데이터의 신뢰성을 보장합니다.\\n   - **UDP**: 비연결 지향 프로토콜로, 연결을 설정하지 않고 데이터를 전송합니다. 이로 인해 전송 속도가 빠르지만, 신뢰성은 낮습니다.\\n\\n2. **신뢰성**:\\n   - **TCP**: 데이터가 손실되거나 순서가 바뀌면 재전송을 요청하여 데이터의 정확성을 보장합니다. 따라서 신뢰성이 높습니다.\\n   - **UDP**: 데이터가 손실되거나 순서가 바뀌어도 이를 확인하지 않으므로 신뢰성이 낮습니다. 실시간 전송이 필요한 경우에 주로 사용됩니다.\\n\\n3. **전송 속도**:\\n   - **TCP**: 연결 설정 및 데이터 확인 과정이 필요하기 때문에 상대적으로 전송 속도가 느릴 수 있습니다.\\n   - **UDP**: 연결 설정이 없고 데이터 확인 과정이 없기 때문에 전송 속도가 빠릅니다.\\n\\n4. **용도**:\\n   - **TCP**: 웹 브라우징, 이메일, 파일 전송 등 데이터의 정확성이 중요한 애플리케이션에서 사용됩니다.\\n   - **UDP**: 온라인 게임, 비디오 스트리밍, VoIP(Voice over IP) 등 실시간 전송이 중요한 애플리케이션에서 사용됩니다.\\n\\n이러한 차이점으로 인해 TCP와 UDP는 각각의 용도에 맞게 선택되어 사용됩니다.', 'userQuestion': 'TCP와 UDP의 차이점은 무엇인가요?'}\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "🧠 Embedding the input question...\n",
      "🔍 Comparing with existing documents...\n",
      "Assistant: {'LLMAnswer': None, 'userQuestion': '데이터베이스에 대해 알려주세요.'}\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "🧠 Embedding the input question...\n",
      "🔍 Comparing with existing documents...\n",
      "\n",
      "가장 유사한 질문: '5\n",
      "2024-08-20\n",
      "스위치와 허브의 차이점은?\n",
      "스위치는 데이터 패킷을 특정 장치로 전달하는 반면, 허브는 모든 장치로 데이터를 전송합니다.\n",
      "6\n",
      "2024-08-20\n",
      "IP 주소란?\n",
      "IP 주소는 네트워크 상에서 장치들을 식별하기 위한 고유 주소입니다.\n",
      "7\n",
      "2024-08-20\n",
      "DNS의 역할은?\n",
      "DNS는 도메인 이름을 IP 주소로 변환하는 시스템입니다.\n",
      "8\n",
      "2024-08-20\n",
      "방화벽이란?\n",
      "방화벽은 네트워크 보안을 위해 외부의 불법적인 접근을 차단하는 시스템입니다.\n",
      "9\n",
      "2024-08-20\n",
      "VPN의 기능은?\n",
      "VPN은 공용 네트워크에서 사설 네트워크를 사용하는 것처럼 보안을 제공하는 기술입니다.\n",
      "10\n",
      "2024-08-20\n",
      "패킷이란?'\n",
      "유사도: 84.24%\n",
      "Assistant: {'LLMAnswer': \"TCP(Transmission Control Protocol)의 이점은 다음과 같습니다:\\n\\n1. **신뢰성**: TCP는 데이터 전송의 신뢰성을 보장합니다. 데이터가 손실되거나 손상되었을 경우, TCP는 해당 데이터를 재전송하여 완전한 데이터 전송을 보장합니다.\\n\\n2. **순서 보장**: TCP는 데이터 패킷이 전송된 순서대로 수신되도록 합니다. 이는 수신 측에서 데이터가 올바른 순서로 조립될 수 있게 해줍니다.\\n\\n3. **흐름 제어**: TCP는 송신자와 수신자 간의 데이터 전송 속도를 조절하여 수신자가 처리할 수 있는 양만큼만 데이터를 전송합니다. 이를 통해 네트워크 혼잡을 방지합니다.\\n\\n4. **혼잡 제어**: TCP는 네트워크의 혼잡 상태를 감지하고, 혼잡할 때 데이터 전송 속도를 조절하여 네트워크의 효율성을 높입니다.\\n\\n5. **연결 지향**: TCP는 데이터 전송을 시작하기 전에 송신자와 수신자 간의 연결을 설정합니다. 이 과정은 '3-way handshake'라고 불리며, 안정적인 통신을 위한 기반을 제공합니다.\\n\\n이러한 이점들 덕분에 TCP는 웹 브라우징, 이메일 전송, 파일 전송 등 신뢰성이 중요한 다양한 애플리케이션에서 널리 사용됩니다.\", 'userQuestion': 'TCP의 이점은 무엇인가요?'}\n",
      "\n",
      "-----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 메인 함수\n",
    "def main(user_input):\n",
    "    response = rag_pipeline(user_input)\n",
    "    print(\"Assistant:\", response)\n",
    "\n",
    "user_input = [\n",
    "                \"MAC에 대해서 자세히 알려주세요.\",\n",
    "                \"컴파일러가 무엇인가요??\", \n",
    "                \"TCP와 UDP의 차이점은 무엇인가요?\", \n",
    "                \"데이터베이스에 대해 알려주세요.\",\n",
    "                \"TCP의 이점은 무엇인가요?\"\n",
    "            ]\n",
    "\n",
    "for i in user_input:\n",
    "    main(i)\n",
    "    print(\"\\n-----------------------------------\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Initializing language model...\n",
      "💾 Setting up cache directory...\n",
      "✂️ Setting up text splitter...\n",
      "📄 Loading document...\n",
      "🔍 Splitting document into chunks...\n",
      "🧠 Initializing embeddings model...\n",
      "🗂️ Setting up cached embeddings...\n",
      "📊 Creating vector store from documents...\n",
      "🔎 Setting up retriever...\n",
      "✅ Process completed!\n",
      "🧠 Embedding the input question...\n",
      "🔍 Comparing with existing documents...\n",
      "Assistant: {'LLMAnswer': 'MAC(Medium Access Control) 주소는 네트워크 인터페이스 카드(NIC)에 할당된 고유한 식별자입니다. 이 주소는 보통 48비트(6바이트) 길이로, 16진수로 표현되며, 일반적으로 다음과 같은 형식으로 나타납니다: `00:1A:2B:3C:4D:5E`.\\n\\nMAC 주소는 네트워크에서 장치를 식별하는 데 사용되며, 주로 로컬 네트워크 내에서 데이터 패킷이 올바른 장치로 전송되도록 돕습니다. 각 네트워크 장치는 고유한 MAC 주소를 가지므로, 네트워크에서 충돌 없이 여러 장치가 동시에 통신할 수 있습니다.\\n\\nMAC 주소는 두 부분으로 나눌 수 있습니다. 첫 번째 3바이트는 OUI(Organizationally Unique Identifier)로, 제조업체를 식별합니다. 나머지 3바이트는 해당 제조업체가 만든 특정 장치를 식별합니다.\\n\\nMAC 주소는 일반적으로 변경되지 않지만, 일부 네트워크 환경에서는 보안이나 관리상의 이유로 소프트웨어적으로 변경할 수 있습니다. MAC 주소는 주로 이더넷 네트워크와 Wi-Fi 네트워크에서 사용됩니다.', 'userQuestion': 'MAC에 대해서 자세히 알려주세요.'}\n",
      "\n",
      "-----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders import UnstructuredFileLoader \n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "from langchain.utils.math import cosine_similarity\n",
    "\n",
    "# 언어 모델 초기화\n",
    "print(\"🔧 Initializing language model...\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
    "\n",
    "# 캐시 디렉토리 설정\n",
    "print(\"💾 Setting up cache directory...\")\n",
    "cache_dir = LocalFileStore(\"./.cache/embeddings/course_computer_networks\")\n",
    "\n",
    "# 텍스트 스플리터 설정\n",
    "print(\"✂️ Setting up text splitter...\")\n",
    "spliter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "# 문서 로드\n",
    "print(\"📄 Loading document...\")\n",
    "loader = UnstructuredFileLoader(\"./dummy.csv\")\n",
    "\n",
    "# 문서 분할\n",
    "print(\"🔍 Splitting document into chunks...\")\n",
    "docs = loader.load_and_split(text_splitter=spliter)\n",
    "\n",
    "# 임베딩 모델 설정\n",
    "print(\"🧠 Initializing embeddings model...\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 캐싱 임베딩 설정\n",
    "print(\"🗂️ Setting up cached embeddings...\")\n",
    "cache_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,\n",
    "    cache_dir,\n",
    ")\n",
    "\n",
    "# 벡터스토어 생성\n",
    "print(\"📊 Creating vector store from documents...\")\n",
    "vectorstore = FAISS.from_documents(docs, cache_embeddings)\n",
    "\n",
    "# 검색기 설정\n",
    "print(\"🔎 Setting up retriever...\")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "print(\"✅ Process completed!\")\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            당신은 AI 어시스턴트입니다. \n",
    "            사용자가 질문에 대한 답변을 찾을 수 있도록 돕습니다.\n",
    "            \n",
    "            사용자 질문과 유사한 질의응답 선 사례 : {context}\n",
    "\n",
    "            위 선사례에 기반하여 사용자의 질문에 대한 답변을 친절하게 설명해주세요.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "def rag_pipeline(question):\n",
    "    # 입력된 질문에 대한 임베딩 생성\n",
    "    print(\"🧠 Embedding the input question...\")\n",
    "    question_embedding = embeddings.embed_documents([question])[0]    \n",
    "    # 기존 질문과 비교\n",
    "    print(\"🔍 Comparing with existing documents...\")\n",
    "    results = retriever.get_relevant_documents(question)\n",
    "    similarities = {}\n",
    "    \n",
    "    for doc in results:\n",
    "        doc_content = doc.page_content.strip()\n",
    "        if not doc_content:\n",
    "            continue\n",
    "        \n",
    "        doc_embedding = embeddings.embed_documents([doc_content])[0]\n",
    "        similarity = cosine_similarity([question_embedding], [doc_embedding])[0][0]\n",
    "        similarity_percentage = similarity * 100\n",
    "        similarities[doc_content] = similarity_percentage\n",
    "\n",
    "    # 가장 유사한 질문 찾기\n",
    "    most_similar_question = max(similarities, key=similarities.get, default=None)\n",
    "    if most_similar_question:\n",
    "        max_similarity = similarities[most_similar_question]\n",
    "        \n",
    "\n",
    "        if max_similarity > 80.0:  # 유사도가 충분히 높은 경우 관련 답변 사용\n",
    "            context = f\"질문이 '{most_similar_question}'와 매우 유사합니다.\"\n",
    "            # print(f\"\\n가장 유사한 질문: '{most_similar_question}'\")\n",
    "            # print(f\"유사도: {max_similarity:.2f}%\")\n",
    "            prompt = prompt_template.format(context=context, question=question)\n",
    "            output = llm.invoke(prompt)\n",
    "            return { \"LLMAnswer\": output.content, \"userQuestion\": question}\n",
    "    \n",
    "    return {\"LLMAnswer\": None, \"userQuestion\": question}\n",
    "\n",
    "# 메인 함수\n",
    "def main(user_input):\n",
    "    response = rag_pipeline(user_input)\n",
    "    print(\"Assistant:\", response)\n",
    "\n",
    "user_input = [\n",
    "                \"MAC에 대해서 자세히 알려주세요.\",\n",
    "                # \"컴파일러가 무엇인가요??\", \n",
    "                # \"TCP와 UDP의 차이점은 무엇인가요?\", \n",
    "                # \"데이터베이스에 대해 알려주세요.\",\n",
    "                # \"TCP의 이점은 무엇인가요?\"\n",
    "            ]\n",
    "\n",
    "for i in user_input:\n",
    "    main(i)\n",
    "    print(\"\\n-----------------------------------\\n\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
